# chapter8

## 开发细节

pycharm2023版本远程连接autodl服务器使用jupyter notebook时如何配置？

断网打开jupyterlab获得地址栏中的地址和token，填入configure server中。

## 8.1 序列模型

### 语法时间

```
time = torch.arange(i, T+1, dtype=torch.float32)
x = torch.sin(0.01*time) + torch.normal(0, 0.2, (T,))
```

arange在很多库中都有实现，给出一个左闭右开的连续序列，normal给出高斯分布（均值0，方差0.2）的随机数，最后一个参数在括号里为什么要加上逗号，表明这个括号是一个元组而非运算符号。

```
range(tau)
```

range给出0到tau-1的连续序列

```python3
def train(net, train_iter, loss, epochs, lr):
	trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
        	trainer.zero_grad()
        	l = loss(net(X))
            l.sum().backward()
            trainer.step()
        print('smothing to say')
        
net = get_net()
train(net, train_iter, loss, 5, 0.01)
```

训练四步：清空上一步微分，计算这一步loss，计算这一分修改，更新这一步修改。

```
x.detach().numpy()
```

对于不需要跟踪计算的操作，使用detach生成一个副本进行操作。

## 8.2 文本预处理

```
with open(d2l.download('time_machine'), 'r') as f:
	lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

lines = read_time_machine()
```

其中的 re.sub() 函数能够通过正则表达式匹配line中的字符执行想要的行为，比如这里的 \[^A-Za-z]的表达式 ^ 表示取反，A-Za-z表示所有大小写字母，也就是说将所有非字母替换为 ’ ‘ 空格符。简单而言，这个函数就是替换函数。

至于 with as 语法，设计出来是为了正确处理文件、网络打开后结束时能被正确地清理，因此在打开文件时尽量选择 with as 语句块。

```python
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_token=None):
        if tokens is None:
            tokens = []
            if reserved_tokens is None:
                reserved_tokens = []
            
            counter = count_corpus(tokens)
            self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
            self.idx_to_token = ['<unk>'] + reserved_tokens
            self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
            for token, freq in self._token_freqs:
                if freq < min_freq:
                    break
                if token not in self.token_to_idx:
                    self.idx_to_token.append(token)
                    self.token_to_idx[token] = len(self.idx_to_token) - 1
            
   	def __len__(self):
        return len(self.idx_to_token)
    
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            
    
    def count_corpus(tokens):
        """count the frequency of words"""
        if len(tokens)==0 or isinstance(tokens[0], list):
            tokens = [token for line in tokens for token in line]
        return collections.Counter(tokens)
```

`counter.items()`：这假设`counter`是一个`Counter`对象（来自`collections`模块），它是一个字典的子类，用于计数可哈希对象。`.items()`方法返回一个包含字典键值对的视图对象，即`(key, value)`元组的列表。在这个上下文中，每个`key`可能代表一个词汇（或标记），而`value`是该词汇的出现频率。

```python
token for line in tokens for token in line
```

从左往右读，先 for line in tokens，对于line	for toke in line	最后出现一个token

### collections 库

```python
import collections

something = ['a', 'a', 'b']
collection = collections.Counter()	# 生成一个Counter对象，用于统计可哈希对象的数量
count_of_a = counter['a']			# 获得'a'的数量
counter.update(['a', 'b'])			# 更新这些元素的统计值
counter.most_common(2)				# 获得最常出现的两个元素
```

### dict[ ] 和 dict.get() 不同之处

get方法允许当()中出现不存在的键值时，不抛出KeyError错误，允许返回默认值

```
value = my_dict.get(key, default=None)
```

## 8.3 语言模型和数据集

计算单词的概率，用以计算出给定前面几个单词后出现某个单词的条件概率。

### 8.3.2 马尔可夫模型与n元语法

对于一个时间度量的序列模型，如果通过前t时间步能够得到下一步的状态，则该序列满足马尔可夫条件

### 8.3.3 自然语言统计

```python
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
```

停用词的概念	stop words

观察到一个现象：词频下降得非常快，次品满足奇普夫定律

对于二元组词元和三元组词元的词频。

```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
```

```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
```

显示一元、二元、三元token的词频对比

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

### 8.3.4 读取长序列数据

## 8.4 循环神经网络

对于小批量输入t时间步d个属性的输入 $X_t(n*d)$，用 $H_t(n*h)$ 表示时间步t的隐藏变量，$H_t$ 是由上一个时间步的输入和上一个时间步的隐藏变量 $H_{t-1}$ 计算得到的，并引入新的权重参数 W_hh

值得注意的是，循环神经院落参数包括隐藏层的权重 W_xh(d * h)  W_hh(h * h) 和偏置。即使是在不同的时间步，循环神经网络也总是使用这些模型参数，因此，循环神经网络的参数开销不会随着时间步的增加而增加。

#### torch.randn

生成随机矩阵

#### param.requires_grad_(True)

原地操作，更改requires_grad属性，在执行反向传播算法是，Pytorch会自动计算该张量相对于损失函数的梯度。

#### 定义\_call_方法

能够让类实例化出的实例具有方法一样的用法

## 8.5 从零开始实现循环神经网络

在notebook顶部的菜单中，选择Kernel -> Restart。 使用快捷键，按下0、0（按下两个零），即在命令模式下按两次“零”

debug：1.W_hh之前定义过了，结果是在cpu上的，遂放弃重定义 2. W_hh 写成了 w_hh

### detach_()

detach_() 和 detach() 的区别：前者原地操作

### 梯度裁剪

将参数的梯度映射到一定范围内，防止梯度过大，降低了拟合速度同时降低了模型坍塌的概率。

```
def grad_clipping(net, theta):
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if
                  p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
```

### 训练网络四步走

1. 清零梯度   zero_grad
2. 推理得差   net
3. 求差反推   loss
4. 更新梯度   step
